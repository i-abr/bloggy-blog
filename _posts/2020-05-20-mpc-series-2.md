---
layout: post
title:  "Part 2: Dynamic Programming and Linear systems"
date:   2020-03-15 16:03:21 -0500
categories: mpc-series
permalink: mpc-series/part2
excerpt: Series of blog posts about model predictive control
author: Ian Abraham
---

# Part 2 of MPC Series #
<img src="https://i.stack.imgur.com/lYqJ3.png" alt="MPC model predictive control">

In this part we are going to talking about MPC where the dynamics of the robot are linear and the task that we want the
robot to do is of a specific quadratic form (I will also have some code for you to try this with). If you are just
joining in and have no idea what I am talking about, I recommend going through the [part 1](http://i-abr.github.io/bloggy-blog/mpc-series/part1)
of this series.


## Linear dynamical system ##

Given the state $s_t$ at a discrete time $t$ and action $a_t$ a robot with linear dynamics is defined as

$$
    s_{t+1} = f(s_t, a_t) = As_t + Ba_t
$$

where $A \in \mathbb{R}^{n\times n}$ and $B \in \mathbb{R}^{n \times m}$ are matrices that tells us how the free
dynamics evolve and how control enter the robotic system. Unfortunately very few robots can be defined this way
and for those that can, the restriction is that the system can be controlled (which you can calculate, but I won't
get into).

## Quadratic objective ##

As mentioned in the previous post, we talked about needing a way to measure how well the robot is doing a task. We are
going to go over the case where the goal for the robot is to drive its states to some target location. If you are
familiar with optimization you will know that one of the things you want is for the objective to be some convex
quadratic form. We can write this kind of objective (known as a tracking objective) as $\ell(s,a)=s^\top Q s + a^\top
R a$ where $Q$ and $R$ are positive semidefinite weight. These

## Some Background Knowledge about how to mathematically treat Robots ##

So the first thing you might want to know is how to define the robot, the state of the robot (its position, velocity,
and more), and the assumptions that we make about the robot. This includes how states are dependent on one another as
well as how the robot is controlled.

The state of the robot is typically given to use as $s_t$ where $s_t \in \mathbb{R}^n$. This tells us that the way we
describe the robot can be given to us as a vector of size $n$ on the real number line. The subscript $t$ tells us that
the value of the state is taken at a specific time $t$ on some clock with a specific frequency (we are going to assume
that the time frequency is $1$ and that $t$ takes on integer values for simplicity). You can also write this in a
continuous time formulation $s(t)$, but we won't get into that because you have to start dealing with more complicated
infinite trajectories (which could be a topic in a later post *remind me*).

The state typically contain things that describe the configuration the robot is in time. Take a $N$ joint robot arm:
your state can include the joint position $\theta_t$ and the joint velocity $\dot{\theta}_t$ making the state equal to

$$
    s_t = \begin{bmatrix} \theta \\ \dot{\theta} \end{bmatrix}_t
$$  

The rule of thumb is to include what you can measure on the robot as part of the state (but it is a bit more complicated
than that). You also want to include the set of states which you allows you to predict the next state in time given the
current state and the physics of the robot. What this means in math is you want the following expression

$$
    s_{t+1} = f(s_t)
$$

where the term $f(s) : \mathbb{R}^n \to \mathbb{R}^n$ takes the state at the current time and tells you what the
measurement of the state is at the next time step. This is also known as the Markov assumption, where the next state
only depends on the current state. The function that the evolves the state, $f(s)$, can be thought of as the dynamics of
the robot which you can derive from first principles using the Euler-Lagrange equations or Newton's equations of motion
(you can even get away with a kinematic set of equations if you assume that dynamics don't matter). Either way your
state should be something that you can predict into the future and measure because the goal of MPC is to get the current
measurement of the state and predict what the state will be if you apply a set of control sequences. Sometimes you won't
have full access to the state in MPC/optimal control and this is called a partially observable state (another topic of a
future video). For now, we will assume we have access to the full state of the robot.

You might be thinking, ok cool, how do we control the state and get the robot to do things? Well, great question! A+,
you get a gold star. The way control enters the robot will depend on the robot itself, but mathematically is defined in
the same way. The control (or action) is commonly written as $a_t \in \mathbb{R}^m$ which is vector of real numbers of
size $m$. For simplicity we will assume the the value of the controls can span the real number line, but there are
control problems where the control is constrained to a bounded space (we won't look at the now). Then the most general
form to include the controls into the robot is to append ++use label++ with the control giving us

$$
    s_{t+1} = f(s_t, a_t)
$$  

where now $f(s,a) : \mathbb{R}^{n\times m} \to \mathbb{R}^n$ which tells us that the next state depends on the current
state and the action that was chosen (this becomes a special case of a Markov decision process when you include the set
of actions). If we were to add noise to this process we would have to consider a distribution of possible states that
the robot can be in due to noise. We will address noisy processes in a later post as developing controllers for such
systems requires a bit more leg work. Some assumptions that are typically required for $f$ is that it is continuous
(does not make the state jump around sharply) and it is differentiable (we can take the derivative of $f$ with respect
to the state and control).

## Defining a robot task ##

What we are going to want to do now is have a way to measure how well the robot is doing at a task that we want the
robot to do. The way we can do this is by creating an objective function $\ell (s, a) : \mathbb{R}^{n\times m} \to
\mathbb{R}$ which takes in the state of the robot and the action and returns a value which indicates how good or bad the
robot is doing (good implies reward function, bad implies cost function). As with the dynamics of the robot, we want
this objective function to be continuous and differentiable.

Starting at a measured state $s_0$ and given set of $T$ actions $a_{0:T-1} = $\{ $a_0, \ldots, a_{T-1}$ \} we can measure how well the
robot can perform the task by applying the action at the state and evolving the state through time using the dynamics
$f(s, a)$. Mathematically, we can write this value as

$$
    \mathcal{J}(a_{0:T-1}, s_0) = \sum_{t=0}^{T-1} \ell(s_t, a_t) \\ \text{ subject to } s_{t+1} = f(s_t, a_t)
$$

where $\mathcal{J}$ is commonly known as the value function. Now, if you chose $\ell$ to be a reward function (measuring
how well the robot does) in order to get the robot to do the task, you will want to maximize this objective subject to
the sequence of controls $a$. The inverse is true if $\ell$ is a cost function (measuring how bad the robot is doing).

## Receding horizon ##

For MPC, we solve this optimization at *every time step* and apply the first action $a_0$ to the robot and recycle the
$a_{1:T-1}$ actions for the next optimization iteration (initializing the last control using some heuristic). This step
can be written down as

$$
    a_{0:T-2} = a_{1:T-1} \\
    a_{T-1} = a_\text{def}
$$

where $a_\text{def}$ is a default initialization for an action. The state of the robot is measured and set to $s_0$
where the dynamics model is used to predict the value $\mathcal{J}$ from the action sequence $a_{0:T-1}$. We call this a
receding horizon problem because the actions in the prediction horizon, $T$, is moving as time flows (see the image at
the top of this post for an example description with slightly different notation). This is essentially the idea behind
the receding horizon MPC approach. Solve the optimization problem every time a robot state measurement is received and
replan the set of actions. This approach is has been shown to be pretty robust to model inaccuracies and sensor noise
since replanning is updated based on the new measurement data. Solving the same problem without moving the horizon and
remeasuring the state of the robot is the same as trajectory optimization


We still need to discuss what should be included in the objective function and the role of dynamics. In the following
post, I will go over the special case where the dynamics of the robot are linear and we have a quadratic objective function.
